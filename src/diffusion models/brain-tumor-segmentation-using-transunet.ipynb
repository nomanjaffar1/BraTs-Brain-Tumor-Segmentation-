{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1299795,"sourceType":"datasetVersion","datasetId":751906}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on the provided description of the BraTS dataset, the labels represent different regions of the tumor as follows:\n\nLabel 4: GD-enhancing tumor (ET) — This is the most aggressive and enhancing tumor region.\nLabel 2: Peritumoral edema (ED) — The edema around the tumor.\nLabel 1: Necrotic and non-enhancing tumor core (NCR/NET) — The necrotic part of the tumor, which does not enhance with contrast.\nUpdated Mask Labels:\n0: Background (non-tumor region).\n1: Necrotic and non-enhancing tumor core (NCR/NET).\n2: Peritumoral edema (ED).\n3: GD-enhancing tumor (ET).\nThe key change is that you are replacing Label 4 (ET) with Label 3 for the updated classification system.\n\nMask Processing Considerations:\nYou need to ensure that your masks have been properly preprocessed and follow the updated label scheme:\n\nBackground: 0\nNCR/NET: 1\nED: 2\nET: 3","metadata":{}},{"cell_type":"markdown","source":"# 1: Imports and Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nimport torch\nimport logging\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torch.cuda.amp import autocast, GradScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:08.188544Z","iopub.execute_input":"2024-12-08T13:36:08.188871Z","iopub.status.idle":"2024-12-08T13:36:23.553075Z","shell.execute_reply.started":"2024-12-08T13:36:08.188842Z","shell.execute_reply":"2024-12-08T13:36:23.552269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2: Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_nifti_file(filepath):\n    \"\"\"Load a NIfTI file (.nii.gz) and return the data as a float32 array.\"\"\"\n    try:\n        data = nib.load(filepath).get_fdata().astype(np.float32)\n        return data\n    except FileNotFoundError:\n        # print(f\"File not found: {filepath}\")\n        return None\n    except Exception as e:\n        # print(f\"Error loading file {filepath}: {str(e)}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.554670Z","iopub.execute_input":"2024-12-08T13:36:23.555218Z","iopub.status.idle":"2024-12-08T13:36:23.560314Z","shell.execute_reply.started":"2024-12-08T13:36:23.555189Z","shell.execute_reply":"2024-12-08T13:36:23.559294Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\ndef normalize_volume(volume):\n    \"\"\"Normalize a 3D volume to the range [0, 1].\"\"\"\n    assert isinstance(volume, np.ndarray), \"Volume must be a numpy array.\"\n    data_scaler = MinMaxScaler()  # Explicitly use MinMaxScaler for data\n    volume_flat = volume.flatten().reshape(-1, 1)\n    volume_scaled_flat = data_scaler.fit_transform(volume_flat)\n    return volume_scaled_flat.reshape(volume.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:23.561625Z","iopub.execute_input":"2024-12-08T13:36:23.562020Z","iopub.status.idle":"2024-12-08T13:36:23.587921Z","shell.execute_reply.started":"2024-12-08T13:36:23.561975Z","shell.execute_reply":"2024-12-08T13:36:23.587079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gamma_correction(volume, gamma_values):\n    \"\"\"Apply gamma correction to specific modalities.\"\"\"\n    assert isinstance(volume, np.ndarray), \"Volume must be a numpy array.\"\n    corrected_volume = np.empty_like(volume)\n    for i, gamma in enumerate(gamma_values):\n        corrected_volume[..., i] = np.power(volume[..., i], gamma)\n    return corrected_volume","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.590174Z","iopub.execute_input":"2024-12-08T13:36:23.590912Z","iopub.status.idle":"2024-12-08T13:36:23.598746Z","shell.execute_reply.started":"2024-12-08T13:36:23.590882Z","shell.execute_reply":"2024-12-08T13:36:23.597909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def window_setting_operation(volume, window_width, window_level):\n    \"\"\"Apply Window Setting Operation (WSO) to the volume.\"\"\"\n    assert isinstance(volume, np.ndarray), \"Volume must be a numpy array.\"\n    U = np.max(volume)\n    W = window_width / U\n    b = U * (window_level / window_width - 0.5)\n\n    # Apply WSO\n    return np.maximum(W * volume + b, 0)","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.599760Z","iopub.execute_input":"2024-12-08T13:36:23.600424Z","iopub.status.idle":"2024-12-08T13:36:23.612245Z","shell.execute_reply.started":"2024-12-08T13:36:23.600375Z","shell.execute_reply":"2024-12-08T13:36:23.611613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_volume(volume, crop_size=(128, 128, 155)):\n    \"\"\"Crop the volume to a specific size.\"\"\"\n    assert isinstance(volume, np.ndarray), \"Volume must be a numpy array.\"\n    assert len(volume.shape) >= 3, \"Volume must have at least 3 dimensions.\"\n    \n    # Check original volume shape\n    original_shape = volume.shape\n\n    # Calculate cropping indices\n    center = np.array(original_shape[:3]) // 2\n    start = center - np.array(crop_size) // 2\n    end = start + np.array(crop_size)\n\n    # Ensure we don't go out of bounds\n    start = np.clip(start, 0, None)\n    end = np.clip(end, None, original_shape[:3])\n\n    # Crop and return the volume\n    return volume[start[0]:end[0], start[1]:end[1], start[2]:end[2]]","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.613184Z","iopub.execute_input":"2024-12-08T13:36:23.613438Z","iopub.status.idle":"2024-12-08T13:36:23.625491Z","shell.execute_reply.started":"2024-12-08T13:36:23.613415Z","shell.execute_reply":"2024-12-08T13:36:23.624688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define the Custom Dataset Class","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass BrainTumorDataset(Dataset):\n    def __init__(self, data_dir, crop_size=(128, 128, 128), transform=None, is_training=True):\n        self.data_dir = data_dir\n        self.crop_size = crop_size  # Updated crop size to reflect new depth\n        self.transform = transform\n        self.is_training = is_training\n        self.cases = os.listdir(data_dir)\n\n    def __len__(self):\n        return len(self.cases)\n\n    def preprocess_case(self, case_path):\n        case = os.path.basename(case_path)\n\n        # Construct paths for modalities\n        t1ce_path = os.path.join(case_path, f'{case}_t1ce.nii')\n        t2_path = os.path.join(case_path, f'{case}_t2.nii')\n        flair_path = os.path.join(case_path, f'{case}_flair.nii')\n        mask_path = os.path.join(case_path, f'{case}_seg.nii') if self.is_training else None\n\n        # Load modalities and check if each file exists\n        try:\n            t1ce = load_nifti_file(t1ce_path)\n            t2 = load_nifti_file(t2_path)\n            flair = load_nifti_file(flair_path)\n            mask = load_nifti_file(mask_path) if self.is_training else None\n        except FileNotFoundError as e:\n            return None, None\n\n        # Ensure all required modalities are loaded\n        modalities = [t1ce, t2, flair]\n        loaded_modalities = [normalize_volume(mod) for mod in modalities if mod is not None]\n        if len(loaded_modalities) < 3:\n            return None, None\n\n        # Apply gamma correction and window setting operation\n        gamma_values = [2.9, 3.2, 1.0]  # T1CE, T2, FLAIR (FLAIR unchanged)\n        corrected_volumes = gamma_correction(np.stack(loaded_modalities, axis=-1), gamma_values)\n        corrected_volumes = window_setting_operation(corrected_volumes, 255, 128)\n\n        # Crop to the required depth (remove first 15 and last 12 slices)\n        corrected_volumes = corrected_volumes[:, :, 15:143]\n\n        # Crop the volume spatially\n        cropped_volume = crop_volume(corrected_volumes, self.crop_size)\n\n        if self.is_training:\n            if mask is not None:\n                mask[mask == 4] = 3   # Reassign label 4 to 3 in the mask.\n                mask = mask[:, :, 15:143]  # Crop the mask to match the depth\n                cropped_mask = crop_volume(mask, self.crop_size)\n                cropped_mask = torch.tensor(cropped_mask, dtype=torch.long).permute(2, 0, 1)\n            else:\n                return None, None  # Skip this case if mask is missing\n        else:\n            cropped_mask = None\n\n        return cropped_volume, cropped_mask\n\n    def __getitem__(self, idx):\n        case_path = os.path.join(self.data_dir, self.cases[idx])\n        image, mask = self.preprocess_case(case_path)\n\n        # If a case is skipped (missing files), try the next item\n        if image is None or (self.is_training and mask is None):\n            return self.__getitem__((idx + 1) % len(self.cases))\n\n        # Convert to torch tensors and permute to (channels, depth, height, width)\n        image = torch.tensor(image, dtype=torch.float32).permute(3, 2, 0, 1)  # Shape (Height, Width, Depth, Channels) -> (Channels, Depth, Height, Width)\n\n        # Apply transformations if provided\n        if self.transform:\n            image, mask = self.transform(image, mask)\n\n        return (image, mask) if self.is_training else image\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.626660Z","iopub.execute_input":"2024-12-08T13:36:23.626936Z","iopub.status.idle":"2024-12-08T13:36:23.638603Z","shell.execute_reply.started":"2024-12-08T13:36:23.626913Z","shell.execute_reply":"2024-12-08T13:36:23.637895Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split the Dataset","metadata":{}},{"cell_type":"code","source":"# Define paths and dataset\ndata_dir = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\ncrop_size = (128, 128, 128)\ndataset = BrainTumorDataset(data_dir, crop_size=crop_size)\n\n# Split into train, validation, and test sets\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n\n# DataLoaders\nbatch_size = 1  # Adjust based on available memory\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:23.639391Z","iopub.execute_input":"2024-12-08T13:36:23.639605Z","iopub.status.idle":"2024-12-08T13:36:23.685110Z","shell.execute_reply.started":"2024-12-08T13:36:23.639584Z","shell.execute_reply":"2024-12-08T13:36:23.684527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check the shape and dtype of one batch from each DataLoader\n# def check_batch_shapes_and_dtypes(loader, name):\n#     for images, masks in loader:\n#         print(f\"{name} - Images shape: {images.shape}, Images dtype: {images.dtype}\")\n#         print(f\"{name} - Masks shape: {masks.shape}, Masks dtype: {masks.dtype}\\n\")\n#         break  # Only check the first batch to inspect dimensions and dtype\n\n# # Checking train, validation, and test set shapes and dtypes\n# check_batch_shapes_and_dtypes(train_loader, \"Train\")\n# check_batch_shapes_and_dtypes(val_loader, \"Validation\")\n# check_batch_shapes_and_dtypes(test_loader, \"Test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:23.686004Z","iopub.execute_input":"2024-12-08T13:36:23.686244Z","iopub.status.idle":"2024-12-08T13:36:23.689764Z","shell.execute_reply.started":"2024-12-08T13:36:23.686222Z","shell.execute_reply":"2024-12-08T13:36:23.688940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Visualize Saved Images","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport matplotlib.colors as mcolors\n\n# Custom colormap for segmentation mask\ntumor_colors = {\n    0: (0.0, 0.0, 0.0),  # Background (black)\n    1: (1.0, 0.0, 0.0),  # Necrotic/non-enhancing tumor core (red)\n    2: (0.0, 1.0, 0.0),  # Peritumoral edema (green)\n    3: (0.0, 0.0, 1.0),  # GD-enhancing tumor (blue)\n}\ncmap = mcolors.ListedColormap([tumor_colors[i] for i in tumor_colors.keys()])\n\ndef visualize_samples_with_colors(train_loader, val_loader):\n    # Visualize 5 training samples\n    print(\"Visualizing 5 Training Samples with Tumor Masks:\")\n    train_count = 0\n    for images, masks in train_loader:\n        for i in range(min(5, images.shape[0])):  # Ensure we only take up to 5 samples\n            image = images[i].cpu().numpy()\n            mask = masks[i].cpu().numpy()\n\n            # Check tensor shape and extract the central slice for each modality\n            if len(image.shape) == 4:  # Shape: (C, D, H, W)\n                central_slice_idx = image.shape[1] // 2  # Get depth slice index\n                t1ce_slice = image[0, central_slice_idx, :, :]  # T1CE modality\n                t2_slice = image[1, central_slice_idx, :, :]    # T2 modality\n                flair_slice = image[2, central_slice_idx, :, :] # FLAIR modality\n            else:\n                raise ValueError(\"Unexpected tensor shape. Expected (C, D, H, W).\")\n\n            # Extract the corresponding segmentation mask slice\n            central_slice_idx_mask = mask.shape[0] // 2  # For 3D mask (D, H, W)\n            mask_slice = mask[central_slice_idx_mask, :, :]\n\n            # Plot the T1CE, T2, FLAIR slices and the corresponding segmentation mask\n            fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n            axs[0].imshow(t1ce_slice, cmap='gray')\n            axs[0].set_title(\"T1CE Central Slice (Train)\")\n            axs[0].axis(\"off\")\n\n            axs[1].imshow(t2_slice, cmap='gray')\n            axs[1].set_title(\"T2 Central Slice (Train)\")\n            axs[1].axis(\"off\")\n\n            axs[2].imshow(flair_slice, cmap='gray')\n            axs[2].set_title(\"FLAIR Central Slice (Train)\")\n            axs[2].axis(\"off\")\n\n            axs[3].imshow(mask_slice, cmap=cmap, vmin=0, vmax=len(tumor_colors) - 1)\n            axs[3].set_title(\"Segmentation Mask (Train)\")\n            axs[3].axis(\"off\")\n\n            plt.show()\n            train_count += 1\n            if train_count == 5:\n                break\n        if train_count == 5:\n            break\n\n    # Visualize 1 validation sample\n    print(\"Visualizing 1 Validation Sample with Tumor Masks:\")\n    for images, masks in val_loader:\n        image = images[0].cpu().numpy()\n        mask = masks[0].cpu().numpy()\n\n        # Check tensor shape and extract the central slice for each modality\n        if len(image.shape) == 4:  # Shape: (C, D, H, W)\n            central_slice_idx = image.shape[1] // 2  # Get depth slice index\n            t1ce_slice = image[0, central_slice_idx, :, :]  # T1CE modality\n            t2_slice = image[1, central_slice_idx, :, :]    # T2 modality\n            flair_slice = image[2, central_slice_idx, :, :] # FLAIR modality\n        else:\n            raise ValueError(\"Unexpected tensor shape. Expected (C, D, H, W).\")\n\n        # Extract the corresponding segmentation mask slice\n        central_slice_idx_mask = mask.shape[0] // 2  # For 3D mask (D, H, W)\n        mask_slice = mask[central_slice_idx_mask, :, :]\n\n        # Plot the T1CE, T2, FLAIR slices and the corresponding segmentation mask\n        fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n        axs[0].imshow(t1ce_slice, cmap='gray')\n        axs[0].set_title(\"T1CE Central Slice (Validation)\")\n        axs[0].axis(\"off\")\n\n        axs[1].imshow(t2_slice, cmap='gray')\n        axs[1].set_title(\"T2 Central Slice (Validation)\")\n        axs[1].axis(\"off\")\n\n        axs[2].imshow(flair_slice, cmap='gray')\n        axs[2].set_title(\"FLAIR Central Slice (Validation)\")\n        axs[2].axis(\"off\")\n\n        axs[3].imshow(mask_slice, cmap=cmap, vmin=0, vmax=len(tumor_colors) - 1)\n        axs[3].set_title(\"Segmentation Mask (Validation)\")\n        axs[3].axis(\"off\")\n\n        plt.show()\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:23.693038Z","iopub.execute_input":"2024-12-08T13:36:23.693281Z","iopub.status.idle":"2024-12-08T13:36:23.706013Z","shell.execute_reply.started":"2024-12-08T13:36:23.693259Z","shell.execute_reply":"2024-12-08T13:36:23.705185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_samples_with_colors(train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:23.706885Z","iopub.execute_input":"2024-12-08T13:36:23.707155Z","iopub.status.idle":"2024-12-08T13:36:37.457314Z","shell.execute_reply.started":"2024-12-08T13:36:23.707132Z","shell.execute_reply":"2024-12-08T13:36:37.456509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define the TRAN U-Net Model","metadata":{}},{"cell_type":"code","source":"# Define the 3D Transformer Encoder\nclass Transformer3DEncoder(nn.Module):\n    def __init__(self, in_channels, embed_dim, depth, num_heads):\n        super(Transformer3DEncoder, self).__init__()\n        \n        # Encoder with 3D CNN\n        self.conv1 = nn.Conv3d(in_channels, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        \n        # Transformer layers\n        self.transformer_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n            for _ in range(depth)\n        ])\n        \n    def forward(self, x):\n        # Apply 3D Conv\n        x = self.conv1(x)\n        x = self.relu(x)\n        \n        # Reshape to [seq_len, batch_size, embed_dim] for transformer\n        B, C, D, H, W = x.shape\n        x = x.flatten(2).permute(2, 0, 1)  # [seq_len, batch_size, embed_dim]\n        \n        # Apply transformer layers\n        for layer in self.transformer_layers:\n            x = layer(x)\n        \n        # Reshape back to [batch_size, channels, depth, height, width]\n        x = x.permute(1, 2, 0).view(B, C, D, H, W)\n        return x\n\n# Define the TransUNet3D architecture\nclass TransUNet3D(nn.Module):\n    def __init__(self, in_channels, out_channels, embed_dim=128, num_heads=8, depth=12):\n        super(TransUNet3D, self).__init__()\n        \n        # Encoder with 3D CNN and Transformer Layer\n        self.encoder = Transformer3DEncoder(in_channels, embed_dim, depth, num_heads)\n        \n        # Decoder layers with 3D convolutions and upsampling\n        self.upconv4 = nn.ConvTranspose3d(embed_dim, 64, kernel_size=2, stride=2)\n        self.decoder4 = self.conv_block(embed_dim + 64, 64)\n\n        self.upconv3 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n        self.decoder3 = self.conv_block(64 + 32, 32)\n\n        self.upconv2 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)\n        self.decoder2 = self.conv_block(32 + 16, 16)\n\n        self.upconv1 = nn.ConvTranspose3d(16, 8, kernel_size=2, stride=2)\n        self.decoder1 = self.conv_block(16 + 8, 8)\n\n        # Final output layer (for 4 classes in segmentation)\n        self.final_conv = nn.Conv3d(8, out_channels, kernel_size=1)\n\n    def conv_block(self, in_channels, out_channels):\n        \"\"\"3D Convolution block with ReLU.\"\"\"\n        return nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        \n    def forward(self, x):\n        # Encoder\n        enc = self.encoder(x)\n        \n        # Decoder path (Upsampling and concatenation)\n        up4 = self.upconv4(enc)\n        up4 = F.interpolate(up4, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)  # Align spatial dims\n        dec4 = self.decoder4(torch.cat([up4, enc], dim=1))\n\n        up3 = self.upconv3(dec4)\n        up3 = F.interpolate(up3, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)  # Align spatial dims\n        dec3 = self.decoder3(torch.cat([up3, dec4], dim=1))\n\n        up2 = self.upconv2(dec3)\n        up2 = F.interpolate(up2, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)  # Align spatial dims\n        dec2 = self.decoder2(torch.cat([up2, dec3], dim=1))\n\n        up1 = self.upconv1(dec2)\n        up1 = F.interpolate(up1, size=(128, 128, 128), mode=\"trilinear\", align_corners=False)  # Align spatial dims\n        dec1 = self.decoder1(torch.cat([up1, dec2], dim=1))\n\n        # Final output\n        output = self.final_conv(dec1)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:37.458687Z","iopub.execute_input":"2024-12-08T13:36:37.459088Z","iopub.status.idle":"2024-12-08T13:36:37.475071Z","shell.execute_reply.started":"2024-12-08T13:36:37.459051Z","shell.execute_reply":"2024-12-08T13:36:37.474186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Accuracy Function","metadata":{}},{"cell_type":"code","source":"def compute_metrics(outputs, labels, num_classes):\n    # Apply softmax to the model's raw output (logits)\n    outputs = torch.nn.functional.softmax(outputs, dim=1)\n\n    # Get the predicted class by taking the argmax over the softmax probabilities\n    predictions = torch.argmax(outputs, dim=1)\n\n    # Flatten the predictions and labels to 1D arrays\n    true_labels = labels.cpu().numpy().flatten()\n    predicted_labels = predictions.cpu().numpy().flatten()\n\n    # Compute confusion matrix\n    cm = confusion_matrix(true_labels, predicted_labels, labels=np.arange(num_classes))\n\n    # Initialize metrics dictionaries\n    precision = {}\n    recall = {}\n    specificity = {}\n    f1 = {}\n    dice = {}\n    accuracy = 0.0\n\n    # Calculate per-class confusion matrix and metrics\n    for i in range(num_classes):\n        tp = cm[i, i]\n        fp = cm[:, i].sum() - tp\n        fn = cm[i, :].sum() - tp\n        tn = cm.sum() - (tp + fp + fn)\n\n        precision[i] = tp / (tp + fp) if tp + fp > 0 else 0.0\n        recall[i] = tp / (tp + fn) if tp + fn > 0 else 0.0\n        specificity[i] = tn / (tn + fp) if tn + fp > 0 else 0.0\n        f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if precision[i] + recall[i] > 0 else 0.0\n        dice[i] = 2 * tp / (2 * tp + fp + fn) if tp + fp + fn > 0 else 0.0\n\n    # Global accuracy\n    accuracy = np.sum(true_labels == predicted_labels) / len(true_labels)\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'specificity': specificity,\n        'dice': dice\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-12-08T13:36:37.476202Z","iopub.execute_input":"2024-12-08T13:36:37.476513Z","iopub.status.idle":"2024-12-08T13:36:37.490356Z","shell.execute_reply.started":"2024-12-08T13:36:37.476488Z","shell.execute_reply":"2024-12-08T13:36:37.489688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"# Device setup\n# Model instantiation\nin_channels = 3  \nout_channels = 4  # Output channels (Segmentation classes)\nembed_dim = 128  # Embedding dimension for transformer\nnum_heads = 2  # Number of transformer heads\ndepth = 2  # Number of transformer layers\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Model setup\nmodel = TransUNet3D(in_channels, out_channels, embed_dim, num_heads, depth)\nmodel = nn.DataParallel(model)  # Multi-GPU training\nmodel = model.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()  # CrossEntropyLoss expects integer labels\noptimizer = optim.Adam(model.parameters(), lr= 0.001)\nscaler = torch.amp.GradScaler(device.type)\n\nnum_epochs = 1\ngrad_accum_steps = 4  # Number of gradient accumulation steps\n# early_stop_patience = 5  # Number of epochs to wait before stopping\n# best_val_loss = float('inf')\n# patience_counter = 0\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:37.491432Z","iopub.execute_input":"2024-12-08T13:36:37.491676Z","iopub.status.idle":"2024-12-08T13:36:38.699956Z","shell.execute_reply.started":"2024-12-08T13:36:37.491653Z","shell.execute_reply":"2024-12-08T13:36:38.698980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\ndef train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, grad_accum_steps):\n    model.train()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    running_precision = 0.0\n    running_recall = 0.0\n    running_f1 = 0.0\n    running_specificity = 0.0\n    running_dice = 0.0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n\n        # Use autocast without the device_type argument\n        with torch.autocast(device_type=\"cuda\"):\n        # with autocast(device_type='cuda' if device.type == 'cuda' else 'cpu'):\n            \n            outputs = model(images)\n            \n            if outputs is None:\n                continue\n\n            # Calculate the loss and metrics\n            loss = criterion(outputs, labels)\n            num_classes=4\n            # You can include metrics here (accuracy, precision, recall, etc.)\n            # Assuming compute_metrics() is defined elsewhere to calculate these metrics\n            metrics = compute_metrics(outputs, labels,num_classes)\n            \n            # Scaler updates\n            scaler.scale(loss).backward()\n            \n            if (batch_idx + 1) % grad_accum_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n            \n            # # Update running metrics\n            running_loss += loss.item()\n          \n           \n            # Average per-class metrics\n            running_accuracy += metrics['accuracy']\n            running_precision += np.mean(list(metrics['precision'].values()))  # Average precision for all classes\n            running_recall += np.mean(list(metrics['recall'].values()))  # Average recall for all classes\n            running_f1 += np.mean(list(metrics['f1'].values()))  # Average F1-score for all classes\n            running_specificity += np.mean(list(metrics['specificity'].values()))  # Average specificity for all classes\n            running_dice += np.mean(list(metrics['dice'].values()))  # Average Dice coefficient for all classes\n             # Clear GPU memory periodically to avoid memory fragmentation\n            if (batch_idx + 1) % grad_accum_steps == 0:\n                torch.cuda.empty_cache()  # Clear cache to avoid fragmentation\n        \n    # Return average metrics for the epoch\n    return (running_loss / len(train_loader),\n            running_accuracy / len(train_loader),\n            running_precision / len(train_loader),\n            running_recall / len(train_loader),\n            running_f1 / len(train_loader),\n            running_specificity / len(train_loader),\n            running_dice / len(train_loader))\n\n# Update validate_one_epoch similarly, ensuring no device_type in autocast()\ndef validate_one_epoch(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_accuracy = 0.0\n    running_precision = 0.0\n    running_recall = 0.0\n    running_f1 = 0.0\n    running_specificity = 0.0\n    running_dice = 0.0\n    \n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Use autocast without the device_type argument\n            with torch.autocast(device_type=\"cuda\"):\n                outputs = model(images)\n                \n                if outputs is None:\n                    continue\n\n                # Calculate the loss and metrics\n                loss = criterion(outputs, labels)\n                num_classes=4\n                metrics = compute_metrics(outputs, labels,num_classes)\n            \n            # Update running metrics\n            running_loss += loss.item()\n            running_accuracy += metrics['accuracy']\n            running_precision += np.mean(list(metrics['precision'].values()))  # Average precision for all classes\n            running_recall += np.mean(list(metrics['recall'].values()))  # Average recall for all classes\n            running_f1 += np.mean(list(metrics['f1'].values()))  # Average F1-score for all classes\n            running_specificity += np.mean(list(metrics['specificity'].values()))  # Average specificity for all classes\n            running_dice += np.mean(list(metrics['dice'].values()))  # Average Dice coefficient for all classes\n        \n    # Return average metrics for the epoch\n    return (running_loss / len(val_loader),\n            running_accuracy / len(val_loader),\n            running_precision / len(val_loader),\n            running_recall / len(val_loader),\n            running_f1 / len(val_loader),\n            running_specificity / len(val_loader),\n            running_dice / len(val_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:38.701467Z","iopub.execute_input":"2024-12-08T13:36:38.702360Z","iopub.status.idle":"2024-12-08T13:36:38.717158Z","shell.execute_reply.started":"2024-12-08T13:36:38.702310Z","shell.execute_reply":"2024-12-08T13:36:38.716391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to track metrics\ntrain_losses, val_losses = [], []\ntrain_accuracies, val_accuracies = [], []\n\n# Directory to save outputs in Kaggle\noutput_dir = '/kaggle/working/'\nos.makedirs(output_dir, exist_ok=True)\n\n# Initialize an empty DataFrame to store metrics\nmetrics_df = pd.DataFrame(columns=[\n    \"Epoch\", \n    \"Train Loss\", \"Validation Loss\", \n    \"Train Accuracy\", \"Validation Accuracy\", \n    \"Train Precision\", \"Validation Precision\",\n    \"Train Recall\", \"Validation Recall\",\n    \"Train F1\", \"Validation F1\",\n    \"Train Specificity\", \"Validation Specificity\",\n    \"Train Dice\", \"Validation Dice\"\n])\n\n# Training loop with tqdm\nprint(\"Tracking epochs with tqdm...\")\nfor epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n    # Training phase\n    train_loss, train_accuracy, train_precision, train_recall, train_f1, train_specificity, train_dice = train_one_epoch(\n        model, train_loader, criterion, optimizer, scaler, device, grad_accum_steps\n    )\n\n    # Validation phase\n    val_loss, val_accuracy, val_precision, val_recall, val_f1, val_specificity, val_dice = validate_one_epoch(\n        model, val_loader, criterion, device\n    )\n\n    # Track metrics\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n\n    # Add metrics to the DataFrame (using pd.concat instead of append)\n    new_row = pd.DataFrame({\n        \"Epoch\": [epoch + 1],\n        \"Train Loss\": [train_loss],\n        \"Validation Loss\": [val_loss],\n        \"Train Accuracy\": [train_accuracy],\n        \"Validation Accuracy\": [val_accuracy],\n        \"Train Precision\": [train_precision],\n        \"Validation Precision\": [val_precision],\n        \"Train Recall\": [train_recall],\n        \"Validation Recall\": [val_recall],\n        \"Train F1\": [train_f1],\n        \"Validation F1\": [val_f1],\n        \"Train Specificity\": [train_specificity],\n        \"Validation Specificity\": [val_specificity],\n        \"Train Dice\": [train_dice],\n        \"Validation Dice\": [val_dice]\n    })\n    metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n\n    # Print metrics for both training and validation\n    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Precision: {train_precision:.4f}, \"\n          f\"Train Recall: {train_recall:.4f}, Train F1: {train_f1:.4f}, Train Specificity: {train_specificity:.4f}, \"\n          f\"Train Dice: {train_dice:.4f}, \"\n          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, \"\n          f\"Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}, Val Specificity: {val_specificity:.4f}, \"\n          f\"Val Dice: {val_dice:.4f}\")\n\n    # Save checkpoint if validation loss improves\n    if epoch+1 == num_epochs:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), os.path.join(output_dir, 'best_model_3D_UNET.pth'))\n        torch.save(model, os.path.join(output_dir, \"entire_model_3D_UNET.pth\"))\n        print(\"Saved best model checkpoint.\")\n\nprint(\"Training completed.\")\n\n# Save metrics DataFrame as a CSV file\ncsv_path = os.path.join(output_dir, 'training_metrics_3D_UNET.csv')\nmetrics_df.to_csv(csv_path, index=False)\nprint(f\"Training metrics saved to {csv_path}.\")\n\n# Plotting the loss and accuracy\nprint(\"Plotting and saving graphs...\")\n\nplt.figure(figsize=(14, 6))\n\n# Plot Loss\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o', color=\"green\")\nplt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o', color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss over Epochs\")\nplt.legend()\nplt.grid()\n\n# Plot Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Train Accuracy\", marker='o', color=\"green\")\nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o', color=\"red\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over Epochs\")\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\n\n# Save the plot\ngraph_path = os.path.join(output_dir, 'training_graph_3D_UNET.png')\nplt.savefig(graph_path)\nplt.show()\n\nprint(f\"Graph saved at {graph_path}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:36:38.718290Z","iopub.execute_input":"2024-12-08T13:36:38.718561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport matplotlib.colors as mcolors\n\n# Define a custom colormap for the segmentation mask\ntumor_colors = {\n    0: (0.0, 0.0, 0.0),  # Background (black)\n    1: (1.0, 0.0, 0.0),  # Necrotic/non-enhancing tumor core (red)\n    2: (0.0, 1.0, 0.0),  # Peritumoral edema (green)\n    3: (0.0, 0.0, 1.0),  # GD-enhancing tumor (blue)\n}\ncmap = mcolors.ListedColormap([tumor_colors[i] for i in tumor_colors.keys()])\n\ndef visualize_results(image, mask, prediction, cmap, tumor_colors):\n    \"\"\"\n    Visualize T1CE, T2, FLAIR modalities, the segmentation mask, and predicted mask.\n    \"\"\"\n    central_slice_idx = image.shape[1] // 2  # Get central slice index for 3D data\n    t1ce_slice = image[0, central_slice_idx, :, :]  # T1CE modality\n    t2_slice = image[1, central_slice_idx, :, :]    # T2 modality\n    flair_slice = image[2, central_slice_idx, :, :] # FLAIR modality\n    mask_slice = mask[central_slice_idx, :, :]      # Ground truth mask\n    pred_slice = prediction[central_slice_idx, :, :]  # Predicted mask\n\n    # Plot the slices\n    fig, axs = plt.subplots(1, 5, figsize=(24, 6))\n    axs[0].imshow(t1ce_slice, cmap='gray')\n    axs[0].set_title(\"T1CE Slice\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(t2_slice, cmap='gray')\n    axs[1].set_title(\"T2 Slice\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(flair_slice, cmap='gray')\n    axs[2].set_title(\"FLAIR Slice\")\n    axs[2].axis(\"off\")\n\n    axs[3].imshow(mask_slice, cmap=cmap, vmin=0, vmax=len(tumor_colors) - 1)\n    axs[3].set_title(\"Segmentation Mask\")\n    axs[3].axis(\"off\")\n\n    axs[4].imshow(pred_slice, cmap=cmap, vmin=0, vmax=len(tumor_colors) - 1)\n    axs[4].set_title(\"Predicted Mask\")\n    axs[4].axis(\"off\")\n\n    plt.show()\n\ndef test_model(model_path, test_loader, device):\n    \"\"\"\n    Test the model and visualize results.\n    \"\"\"\n    # Load the saved model\n    model = torch.load(model_path, map_location=device)\n    model.eval()\n\n    with torch.no_grad():\n        for images, masks in test_loader:\n            images, masks = images.to(device), masks.cpu().numpy()\n            \n            # Predict and visualize for the first batch\n            predictions = model(images)\n            predictions = torch.argmax(predictions, dim=1).cpu().numpy()  # Convert predictions to class indices\n            \n            for i in range(min(5, images.shape[0])):  # Visualize up to 5 samples\n                image = images[i].cpu().numpy()  # Convert image to numpy\n                mask = masks[i]\n                prediction = predictions[i]\n                visualize_results(image, mask, prediction, cmap, tumor_colors)\n            \n\n# Example usage\nif __name__ == \"__main__\":\n    # Specify the saved model path\n    saved_model_path = \"entire_model_3D_UNET.pth\"\n\n    # Assume `test_loader` and `device` are already defined\n    test_model(saved_model_path, test_loader, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef count_parameters(model):\n    \"\"\"\n    Count the total number of parameters in the model.\n    \n    Args:\n        model (torch.nn.Module): The PyTorch model.\n        \n    Returns:\n        int: Total number of parameters in the model.\n    \"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    return total_params\n\n# Load your model\nmodel = torch.load(saved_model_path)\nmodel = model.to(device)  # Ensure the model is on the correct device\n\n# Print the total number of parameters\ntotal_params = count_parameters(model)\nprint(f'Total Parameters in the Model: {total_params}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}